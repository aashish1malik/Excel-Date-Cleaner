{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a508c3",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a95812b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'Let', \"'s\", 'explore', 'it', '.']\n",
      "Sentence Tokens: ['Natural Language Processing (NLP) is amazing!', \"Let's explore it.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Natural Language Processing (NLP) is amazing! Let's explore it.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a53f93",
   "metadata": {},
   "source": [
    "# Lowercasing in NLP (Text Preprocessing Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d849d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '!', 'nlp', 'is', 'fun', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hello World! NLP is Fun.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lowercasing each token\n",
    "lower_tokens = [word.lower() for word in tokens]\n",
    "print(lower_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cb0b1",
   "metadata": {},
   "source": [
    "# Example 4: Lowercasing Using pandas (Useful for Large Text Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97abc61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Text    Lowercase_Text\n",
      "0      HELLO World!      hello world!\n",
      "1      THIS is NLP.      this is nlp.\n",
      "2  Machine Learning  machine learning\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({'Text': [\"HELLO World!\", \"THIS is NLP.\", \"Machine Learning\"]})\n",
    "data['Lowercase_Text'] = data['Text'].str.lower()\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9496e",
   "metadata": {},
   "source": [
    "# STOP WORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c653757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['simple', 'example', 'demonstrate', 'stopword', 'removal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text = \"This is a simple example to demonstrate stopword removal.\"\n",
    "word = text.split()\n",
    "filtered_words = [word for word in word if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d9245",
   "metadata": {},
   "source": [
    "# 3. Stemming (Reducing Words to Their Root Form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82caeeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'amaz', '!', 'let', \"'s\", 'explor', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in word_tokens]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73db13",
   "metadata": {},
   "source": [
    "# 4. Lemmatization (More Advanced Root Word Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f47065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'amazing', '!', 'Let', \"'s\", 'explore', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4038143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b1415",
   "metadata": {},
   "source": [
    "# 5. Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08c5a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('amazing', 'JJ'), ('!', '.'), ('Let', 'NNP'), (\"'s\", 'POS'), ('explore', 'VB'), ('it', 'PRP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d6247e",
   "metadata": {},
   "source": [
    "# 6. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4594ca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: (S\n",
      "  Natural/JJ\n",
      "  Language/NNP\n",
      "  Processing/NNP\n",
      "  (/(\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  amazing/JJ\n",
      "  !/.\n",
      "  Let/NNP\n",
      "  's/POS\n",
      "  explore/VB\n",
      "  it/PRP\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\", ner_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7344d28",
   "metadata": {},
   "source": [
    "# 7. Frequency Distribution of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05a89c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words: [('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1), ('NLP', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(word_tokens)\n",
    "print(\"Most Common Words:\", fdist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f835b9",
   "metadata": {},
   "source": [
    "# 8. Text Classification using Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5183e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3802369e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\KIET\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.735\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     13.9 : 1.0\n",
      "               insulting = True              neg : pos    =     13.7 : 1.0\n",
      "              vulnerable = True              pos : neg    =     13.0 : 1.0\n",
      "               ludicrous = True              neg : pos    =     12.6 : 1.0\n",
      "             uninvolving = True              neg : pos    =     12.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Prepare the dataset\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "positive_reviews = [(extract_features(movie_reviews.words(fileid)), 'pos') \n",
    "                    for fileid in movie_reviews.fileids('pos')]\n",
    "negative_reviews = [(extract_features(movie_reviews.words(fileid)), 'neg') \n",
    "                    for fileid in movie_reviews.fileids('neg')]\n",
    "\n",
    "train_data = positive_reviews[:800] + negative_reviews[:800]\n",
    "test_data = positive_reviews[800:] + negative_reviews[800:]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# Test the classifier\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_data))\n",
    "\n",
    "# Show the most informative features\n",
    "classifier.show_most_informative_features(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a3739",
   "metadata": {},
   "source": [
    "# Working with BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767f95e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 17662, 2227, 3084, 17953, 2361, 3733, 1998, 8114, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Hugging Face makes NLP easy and efficient!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8207c8c",
   "metadata": {},
   "source": [
    "# Detailed Tokenization Steps-Tokenization- Convert text into tokens (subword units).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a795b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hugging', 'face', 'makes', 'nl', '##p', 'easy', 'and', 'efficient', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018cc80",
   "metadata": {},
   "source": [
    "# Converting token into IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e78c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [17662, 2227, 3084, 17953, 2361, 3733, 1998, 8114, 999]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e5235",
   "metadata": {},
   "source": [
    "# Add special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca10dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with Special Tokens: [101, 17662, 2227, 3084, 17953, 2361, 3733, 1998, 8114, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "tokens_with_special = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(\"Tokens with Special Tokens:\", tokens_with_special)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8361aa4",
   "metadata": {},
   "source": [
    "# Generate Attention Masks - Attention masks tell the model which tokens to focus on (1) and which to ignore (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facbdb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 17662,  2227,  3084, 17953,  2361,  3733,  1998,  8114,   999,\n",
      "           102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs:\", encoded[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encoded[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b8986",
   "metadata": {},
   "source": [
    "# Using BERT Tokenizer for Multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b2550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 19081,  2024,  6429,  1012,   102,     0,     0,     0,     0],\n",
      "        [  101,  2027,  2024,  2109,  1999, 17953,  2361,  8077,  1012,   102]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Transformers are amazing.\", \"They are used in NLP extensively.\"]\n",
    "\n",
    "# Tokenize multiple sentences\n",
    "encoded_batch = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs:\", encoded_batch[\"input_ids\"])\n",
    "print(\"Attention Mask:\", encoded_batch[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b043018",
   "metadata": {},
   "source": [
    "# Working with BERT Tokenizer for a Pair of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de6482eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  101, 14324,  2003,  2307,  2005, 17953,  2361,  1012,   102,  2009,\n",
      "          3594, 19081,  2000,  3305,  6123,  1012,   102]])\n",
      "Token Type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "text1 = \"BERT is great for NLP.\"\n",
    "text2 = \"It uses transformers to understand context.\"\n",
    "\n",
    "# Encode a pair of sentences\n",
    "encoded_pair = tokenizer(text1, text2, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs:\", encoded_pair[\"input_ids\"])\n",
    "print(\"Token Type IDs:\", encoded_pair[\"token_type_ids\"])  # Differentiates Sentence 1 and 2\n",
    "print(\"Attention Mask:\", encoded_pair[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876645b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
